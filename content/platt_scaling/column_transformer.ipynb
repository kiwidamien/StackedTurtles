{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T21:05:33.267104Z",
     "start_time": "2019-05-24T21:05:33.260320Z"
    }
   },
   "source": [
    "Here are some examples of common preprocessing steps on different types of features:\n",
    "* __Imputation__: Replacing missing values with the mean or median,\n",
    "* __Log transform__: Taking the log of right-skewed numeric features, or its more general cousin, the _Box-Cox transform_.\n",
    "* __Scaling__: Standardizing numeric features so values are on similar scales,\n",
    "* __Encoding__: Transforming categorical variables to numeric features.\n",
    "\n",
    "Some of these methods, such as taking a log transform, can be done on the full dataset as they don't require estimation of any parameters from the dataset. Methods such as standard scaling require an estimate of the mean and variance from the data, and imputation requires an estimate on the mean and median. Technically, these steps cannot be done on the full dataset. It is easy to `fit` on the training data, and then `transform` on the test data. Slightly more subtle is the issue of cross-validation: these steps should really be `fit` separately on the training _folds_, and `tranform`ed on the validation fold. Otherwise we are getting some information from the validation set for our preprocessing.\n",
    "\n",
    "As a _practical_ matter, at least with standard scaler and imputation, the validation set shouldn't have a huge effect on the mean, median, or variance. More importantly, if you want to scale some variables, and log transform others (for example) it is tedious to select the features you want to transform, do the transform, and then paste the transformed features back together.\n",
    "\n",
    "A recent addition to Scikit-learn is the `ColumnTransformer`, which allows us to specify different transformations per column, making preprocessing a lot less tedious (and a lot easier to read!). As a bonus, it is easy to put a `ColumnTransformer` into a pipeline, and do the scaling and imputation correctly. There is a [nice example of doing this](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) in the documentation; this article is a more detailed walkthrough of that example.\n",
    "\n",
    "In this article, we will first walk through a fairly simple example to see how to use a `ColumnTransformer`, and then a more involved example to really see how they are valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example: predict gender from height, weight, and colorblindness\n",
    "\n",
    "In this example, we are going to make a simple predictor for predicting gender based on someone's weight (in pounds), height (in inches), and whether or not they are colorblind. It is difficult to think of an application of this model where we know someone's weight, height, and information about their vision but _don't_ know his or her gender, but it is a simple example with only a few features.\n",
    "\n",
    "This dataset has 10k rows; let's look at the first few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T16:01:24.429448Z",
     "start_time": "2019-05-25T16:01:22.836976Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:18.045958Z",
     "start_time": "2019-05-25T17:50:18.041092Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:18.977087Z",
     "start_time": "2019-05-25T17:50:18.939563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Colorblind</th>\n",
       "      <th>is_female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>145.576248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.486412</td>\n",
       "      <td>172.546072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.505927</td>\n",
       "      <td>206.440942</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>122.034650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.488671</td>\n",
       "      <td>135.013164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Height      Weight  Colorblind  is_female\n",
       "0        NaN  145.576248         0.0       True\n",
       "1  68.486412  172.546072         0.0      False\n",
       "2  70.505927  206.440942         0.0      False\n",
       "3        NaN  122.034650         0.0       True\n",
       "4  65.488671  135.013164         NaN       True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore\n",
    "people = (pd.read_csv('weight-height-color.csv')\n",
    "          .sample(frac=1, random_state=14)\n",
    "          .reset_index(drop=True)\n",
    "          .rename(columns={'colorblind': 'Colorblind'})\n",
    "         )\n",
    "h_mask = (np.random.random(size=len(people)) < 0.06)\n",
    "c_mask = (np.random.random(size=len(people)) < 0.05)\n",
    "\n",
    "people.loc[h_mask, 'Height'] = np.nan\n",
    "people.loc[c_mask, 'Colorblind'] = np.nan\n",
    "people['is_female'] = (people.Gender == 'Female')\n",
    "people.drop('Gender', axis=1, inplace=True)\n",
    "\n",
    "X, y = people.drop('is_female', axis=1), people['is_female']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=44)\n",
    "\n",
    "people[['Height', 'Weight', 'Colorblind', 'is_female']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we have some missing values that we will need to fill. We will use logistic regression, which is a regularized model. So we have two things we need to do:\n",
    "* Impute missing values\n",
    "* Standard scale the continuous features, height and weight\n",
    "\n",
    "Let's look at the number of missing values that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:19.659417Z",
     "start_time": "2019-05-25T17:50:19.649842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>num_missing</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Height</th>\n",
       "      <td>float64</td>\n",
       "      <td>597</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Colorblind</th>\n",
       "      <td>float64</td>\n",
       "      <td>527</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_female</th>\n",
       "      <td>bool</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               type  num_missing  total\n",
       "Height      float64          597  10000\n",
       "Weight      float64            0  10000\n",
       "Colorblind  float64          527  10000\n",
       "is_female      bool            0  10000"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore\n",
    "pd.DataFrame({'type': people.dtypes, 'num_missing': people.isna().sum(axis=0), 'total': len(people)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that we don't have any missing weights! But in an ideal world, our imputation strategy would be different for `Height` and `Colorblind`:\n",
    "* For `Height` it is natural to choose the mean or median\n",
    "* For `Colorblind`, it is natural to choose the mode (choosing the mean would give a number that was not 0 or 1!)\n",
    "\n",
    "We could do it in \"one shot\" by picking the median, because for the special case of a binary classifier the median is also the mode. Let's replace the `Height` with the mean instead, to capture some of the difficulties that would be present in more complicated examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Apply same tranformations to all columns\n",
    "\n",
    "One way of approach this problem is to just to a mean imputation and standard scaling on all the columns, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:20.312551Z",
     "start_time": "2019-05-25T17:50:20.303620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46886948e+00, -1.66737018e+00, -2.24024562e-01],\n",
       "       [-9.90723725e-01, -5.60672626e-01, -2.24024562e-01],\n",
       "       [-6.36856723e-01, -6.68249704e-01, -2.24024562e-01],\n",
       "       [ 5.72957617e-01,  9.31586346e-01,  3.42226819e-17]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "pipeline_steps = [\n",
    "    ('mean_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "]\n",
    "\n",
    "preprocess_pipeline = Pipeline(pipeline_steps)\n",
    "\n",
    "X_train_scaled = preprocess_pipeline.fit_transform(X_train)\n",
    "\n",
    "# Look at the first 4 rows\n",
    "X_train_scaled[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of this approach is that the pipeline makes it easy to apply these steps to the test data. We can also put our pipeline into GridSearch, and it will automatically do imputation and scaling using only the training folds (i.e. no data leakage from the validation set to the training set).\n",
    "\n",
    "Some of the downsides to this approach:\n",
    "* We were lucky that we could get away with standard scaling all the columns in this case. \n",
    "* Once we scaled our examples, we lost the binary nature of the colorblind feature. This is going to make it harder to interpret the last column.\n",
    "* Because we did mean imputation, the missing colorblind values were replaced with the mean (i.e. not 0 or 1) before scaling. This means there are three distinct values in this column: actually colorblind, imputed colorblind, and not colorblind. Looking at the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:20.966473Z",
     "start_time": "2019-05-25T17:50:20.956120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled Colorblind Value</th>\n",
       "      <th>Observations in Train set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.240246e-01</td>\n",
       "      <td>6788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.422268e-17</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.707984e+00</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Scaled Colorblind Value  Observations in Train set\n",
       "0            -2.240246e-01                       6788\n",
       "1             3.422268e-17                        389\n",
       "2             4.707984e+00                        323"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore\n",
    "(pd.DataFrame(X_train_scaled, columns=['Height', 'Weight', 'Colorblind'])['Colorblind']\n",
    " .value_counts()\n",
    " .reset_index()\n",
    " .rename(columns={'index': 'Scaled Colorblind Value', 'Colorblind': 'Observations in Train set'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still constuct a Logistic Regression model from this, even if we would have preferred not to do mean imputation on the colorblind values. We can compose the preprocessing pipeline and do a grid search for the appropriate amount of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:22.166278Z",
     "start_time": "2019-05-25T17:50:21.619695Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Do preprocessing and modeling in one step. Pipelines can be attached to\n",
    "# one another!\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "param = {\n",
    "    'classifier__C': [1e-4, 1e-2, 1e-1, 1, 3, 10, 30, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, param_grid=param, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# How well do we do on the validation set vs the test set?\n",
    "# Note we don't need to separately scale / impute the test set\n",
    "test_score = grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:22.833064Z",
     "start_time": "2019-05-25T17:50:22.829628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation set accuracy = 92.21%; the test set accuracy = 91.84%\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(f'The validation set accuracy = {100*grid.best_score_:4.2f}%; the test set accuracy = {100*test_score:4.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: Using the column transformer\n",
    "\n",
    "Let's see how we can use the `ColumnTransformer` to address the same problem. We will look at two types of processing steps:\n",
    "* For the heights and weights, we will do mean imputation, followed by standard scaling.\n",
    "* For the colorblindness, we will only do mode imputation\n",
    "\n",
    "Technically we don't need any imputation on the weights for this dataset, but it is nice to include it incase we fit to future data where the weights are missing. \n",
    "\n",
    "The `ColumnTransformer` works in a similar way to a pipeline, where you feed it a list of tuples. Each tuple contains the _name_ of the step, the _transformation_ you want to do, and a _list of columns_ you want to apply that transformation to. It is this last step that makes it different from an ordinary pipeline. Let's see it in action: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:23.524817Z",
     "start_time": "2019-05-25T17:50:23.511237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.46886948, -1.66737018,  0.        ],\n",
       "       [-0.99072372, -0.56067263,  0.        ],\n",
       "       [-0.63685672, -0.6682497 ,  0.        ],\n",
       "       [ 0.57295762,  0.93158635,  0.        ]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "cnts_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "    \n",
    "colorblind_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "    \n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "    ('continuous', cnts_pipeline, ['Height', 'Weight']),\n",
    "    ('colorblind', colorblind_pipeline, ['Colorblind'])\n",
    "])\n",
    "    \n",
    "X_train_processed = preprocess_pipeline.fit_transform(X_train)\n",
    "X_train_processed[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ColumnTransformer` has applied the `cnts_pipeline` to the columns `Height` and `Weight`, and applied the `colorblind_pipeline` to the `Colorblind` column. Note that the output at the end has the _same_ scaled heights and weights, while preserving the binary nature of the colorblind column.\n",
    "\n",
    "This preprocessing is a little longer than the previous version, but we have been able to specify exactly what we want done to each column rather than compromising for convinience. We can now use the exact same code as above to make our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:25.121138Z",
     "start_time": "2019-05-25T17:50:24.167073Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Do preprocessing and modeling in one step. Pipelines can be attached to\n",
    "# one another!\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocess_pipeline),\n",
    "    ('classifier', LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "param = {\n",
    "    'classifier__C': [1e-4, 1e-2, 1e-1, 1, 3, 10, 30, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, param_grid=param, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# How well do we do on the validation set vs the test set?\n",
    "# Note we don't need to separately scale / impute the test set\n",
    "test_score = grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:25.775371Z",
     "start_time": "2019-05-25T17:50:25.772327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation set accuracy = 92.21%; the test set accuracy = 91.80%\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "print(f'The validation set accuracy = {100*grid.best_score_:4.2f}%; the test set accuracy = {100*test_score:4.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can use the `ColumnTransformer` on a less trivial example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T14:33:03.527700Z",
     "start_time": "2019-05-25T14:33:03.397242Z"
    }
   },
   "source": [
    "## Loan data\n",
    "\n",
    "We will try and find the rate people pay loans back using a Logistic Regression model. Unlike tree based models, this will require us to scale numerical features. This was originally LendingClub data, but to keep the problem simple, some of the features have been binarized, and the categorical features have been dropped. The dataset we will be using is available [here](https://raw.githubusercontent.com/kiwidamien/StackedTurtles/master/content/platt_scaling/lending_club_clean_and_processed.csv). Let's start by looking at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:26.448553Z",
     "start_time": "2019-05-25T17:50:26.422705Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "loans = pd.read_csv('lending_club_clean.csv')\n",
    "#loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:27.171528Z",
     "start_time": "2019-05-25T17:50:27.169189Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "#sns.pairplot(loans.select_dtypes(include=[np.number]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:27.871058Z",
     "start_time": "2019-05-25T17:50:27.868746Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "# loans.pub_rec_bankruptcies.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:28.576811Z",
     "start_time": "2019-05-25T17:50:28.566212Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "if 'term' in loans.columns:\n",
    "    loans['term_5yrs'] = (loans['term'] == 60).astype(bool)\n",
    "    loans.drop('term', axis=1, inplace=True)\n",
    "    \n",
    "loans['emp_length'] = loans['emp_length'].astype(float)\n",
    "\n",
    "if 'pub_rec_bankruptcies' in loans.columns:\n",
    "    loans['has_bankruptcy'] = (loans['pub_rec_bankruptcies'] > 0).astype(bool)\n",
    "    loans.drop('pub_rec_bankruptcies', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:29.295987Z",
     "start_time": "2019-05-25T17:50:29.289292Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "feature_list = list(loans.select_dtypes(include=[np.number, bool]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:30.102525Z",
     "start_time": "2019-05-25T17:50:30.017221Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "to_drop = ['pub_rec', 'revol_bal', 'mths_since_last_delinq', 'total_acc']\n",
    "feature_list = [c for c in feature_list if c not in to_drop]\n",
    "loans[feature_list].to_csv('lending_club_clean_and_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:30.839493Z",
     "start_time": "2019-05-25T17:50:30.820760Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "loans = pd.read_csv('lending_club_clean_and_processed.csv')\n",
    "features = loans.drop('defaulted', axis=1)\n",
    "target = loans.defaulted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:31.481277Z",
     "start_time": "2019-05-25T17:50:31.475639Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:32.293650Z",
     "start_time": "2019-05-25T17:50:32.278677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>term_5yrs</th>\n",
       "      <th>has_bankruptcy</th>\n",
       "      <th>defaulted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>84000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>50004.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.97</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.50</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>24044.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  annual_inc  open_acc    dti  delinq_2yrs  inq_last_6mths  \\\n",
       "0    10000.0     49200.0      10.0  20.00          0.0             1.0   \n",
       "1     3000.0     80000.0      15.0  17.94          0.0             0.0   \n",
       "2     6000.0     84000.0       4.0  18.44          2.0             0.0   \n",
       "3     5000.0     50004.0      14.0  13.97          3.0             0.0   \n",
       "4     5000.0     24044.0       8.0  11.93          0.0             0.0   \n",
       "\n",
       "   revol_util  emp_length  term_5yrs  has_bankruptcy  defaulted  \n",
       "0       21.00        10.0      False           False      False  \n",
       "1       53.90         1.0       True           False      False  \n",
       "2       37.73         1.0      False           False      False  \n",
       "3       59.50         2.0       True           False       True  \n",
       "4       29.30         2.0      False           False      False  "
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore\n",
    "col_order = [c for c in loans.columns if c != 'defaulted'] + ['defaulted']\n",
    "loans[col_order].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data doesn't have any missing values, but it has dollar amounts, which can be heavily right-skewed (i.e. the have a bump on the _left_ side). It isn't obvious what some of these columns are, so it makes sense to get the definition of each of the features, and a measure of how skewed they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:33.118103Z",
     "start_time": "2019-05-25T17:50:33.079591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_97e85f3a_7f15_11e9_81bd_acde48001122row0_col2 {\n",
       "            color:  red;\n",
       "        }    #T_97e85f3a_7f15_11e9_81bd_acde48001122row1_col2 {\n",
       "            color:  red;\n",
       "        }    #T_97e85f3a_7f15_11e9_81bd_acde48001122row2_col2 {\n",
       "            color:  red;\n",
       "        }    #T_97e85f3a_7f15_11e9_81bd_acde48001122row4_col2 {\n",
       "            color:  red;\n",
       "        }    #T_97e85f3a_7f15_11e9_81bd_acde48001122row5_col2 {\n",
       "            color:  red;\n",
       "        }</style><table id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Type</th>        <th class=\"col_heading level0 col1\" >Description</th>        <th class=\"col_heading level0 col2\" >skew</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row0\" class=\"row_heading level0 row0\" >loan_amnt</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row0_col0\" class=\"data row0 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row0_col1\" class=\"data row0 col1\" >Amount of loan ($)</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row0_col2\" class=\"data row0 col2\" >1.13328</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row1\" class=\"row_heading level0 row1\" >annual_inc</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row1_col0\" class=\"data row1 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row1_col1\" class=\"data row1 col1\" >Annual income ($)</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row1_col2\" class=\"data row1 col2\" >8.14486</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row2\" class=\"row_heading level0 row2\" >open_acc</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row2_col0\" class=\"data row2 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row2_col1\" class=\"data row2 col1\" >How long account has been open (months)</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row2_col2\" class=\"data row2 col2\" >1.03854</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row3\" class=\"row_heading level0 row3\" >dti</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row3_col0\" class=\"data row3 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row3_col1\" class=\"data row3 col1\" >Debt-to-income ratio as percentage (e.g. 11% -> 11.0)</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row3_col2\" class=\"data row3 col2\" >-0.0436909</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row4\" class=\"row_heading level0 row4\" >delinq_2yrs</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row4_col0\" class=\"data row4 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row4_col1\" class=\"data row4 col1\" >The number of times the borrower had been 30+ days past due on a payment in the past 2 years.</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row4_col2\" class=\"data row4 col2\" >2.99947</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row5\" class=\"row_heading level0 row5\" >inq_last_6mths</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row5_col0\" class=\"data row5 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row5_col1\" class=\"data row5 col1\" >The borrower's number of inquiries by creditors in the last 6 months</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row5_col2\" class=\"data row5 col2\" >2.24966</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row6\" class=\"row_heading level0 row6\" >revol_util</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row6_col0\" class=\"data row6 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row6_col1\" class=\"data row6 col1\" >The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available).</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row6_col2\" class=\"data row6 col2\" >-0.0659327</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row7\" class=\"row_heading level0 row7\" >emp_length</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row7_col0\" class=\"data row7 col0\" >float64</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row7_col1\" class=\"data row7 col1\" >Number of months with current employer</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row7_col2\" class=\"data row7 col2\" >0.193894</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row8\" class=\"row_heading level0 row8\" >term_5yrs</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row8_col0\" class=\"data row8 col0\" >bool</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row8_col1\" class=\"data row8 col1\" >Whether this was a 5 year loan (dataset only had 5 yr and 3 yr loans)</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row8_col2\" class=\"data row8 col2\" >1.11532</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122level0_row9\" class=\"row_heading level0 row9\" >has_bankruptcy</th>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row9_col0\" class=\"data row9 col0\" >bool</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row9_col1\" class=\"data row9 col1\" >Whether the borrower has ever declared bankruptcy</td>\n",
       "                        <td id=\"T_97e85f3a_7f15_11e9_81bd_acde48001122row9_col2\" class=\"data row9 col2\" >3.96065</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a1db878d0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore\n",
    "data_dict = pd.DataFrame(X_train.dtypes, columns=['Type'])\n",
    "data_dict['Description'] = pd.Series({\n",
    "    'loan_amnt': 'Amount of loan ($)',\n",
    "    'annual_inc': 'Annual income ($)',\n",
    "    'open_acc': 'How long account has been open (months)',\n",
    "    'dti': 'Debt-to-income ratio as percentage (e.g. 11% -> 11.0)',\n",
    "    'delinq_2yrs': 'The number of times the borrower had been 30+ days past due on a payment in the past 2 years.',\n",
    "    'inq_last_6mths': 'The borrower\\'s number of inquiries by creditors in the last 6 months',\n",
    "    'revol_util': 'The borrower’s revolving line utilization rate (the amount of the credit line used relative to total credit available).',\n",
    "    'emp_length': 'Number of months with current employer',\n",
    "    'term_5yrs': 'Whether this was a 5 year loan (dataset only had 5 yr and 3 yr loans)',\n",
    "    'has_bankruptcy': 'Whether the borrower has ever declared bankruptcy'\n",
    "})\n",
    "\n",
    "data_dict['skew'] = X_train.skew()\n",
    "\n",
    "\n",
    "data_dict.style.apply(lambda x: [\"color: red\" if (type(v) == float) and (x[0] != 'bool') and (v > 0.9) else \"\" for v in x], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:16:25.094373Z",
     "start_time": "2019-05-25T17:16:25.078019Z"
    }
   },
   "source": [
    "For our processing, we want to:\n",
    "* Log the highly skewed values\n",
    "* Standard scale the numerical values we don't log (logs usually compress the range of a variable enough that we don't need to scale the logged values)\n",
    "* Keep the boolean values untouched\n",
    "\n",
    "To do this, we will make two `ColumnTransformers`: one of the highly skewed features, and another for the remaining numerical (but non-boolean) features. First let's store the skews in a `Series`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:33.941303Z",
     "start_time": "2019-05-25T17:50:33.932247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loan_amnt         1.133279\n",
       "annual_inc        8.144861\n",
       "open_acc          1.038542\n",
       "dti              -0.043691\n",
       "delinq_2yrs       2.999472\n",
       "inq_last_6mths    2.249655\n",
       "revol_util       -0.065933\n",
       "emp_length        0.193894\n",
       "dtype: float64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_skew = X_train.select_dtypes(include=[np.number]).skew()\n",
    "feature_skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to break our features into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:34.776112Z",
     "start_time": "2019-05-25T17:50:34.769752Z"
    }
   },
   "outputs": [],
   "source": [
    "log_features = feature_skew[abs(feature_skew) > 0.9].index\n",
    "scale_features = [name for name in feature_skew.index if name not in log_features]\n",
    "boolean_features = X_train.select_dtypes(exclude=[np.number]).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We could also manually specify which columns we want to be in each group. This would be equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:35.546660Z",
     "start_time": "2019-05-25T17:50:35.543033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "log_features     = ['loan_amnt', 'annual_inc', 'open_acc', 'delinq_2yrs', 'inq_last_6mths']\n",
      "scale_features   = ['dti', 'revol_util', 'emp_length']\n",
      "boolean_features = ['term_5yrs', 'has_bankruptcy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ignore\n",
    "msg=f\"\"\"\n",
    "log_features     = {list(log_features)}\n",
    "scale_features   = {scale_features}\n",
    "boolean_features = {list(boolean_features)}\n",
    "\"\"\"\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one more obstacle before using our `ColumnTransformer`, which is how to take the log of the `log_features`.  We are familiar with `StandardScaler`, but there isn't a build in transformer for taking the `log` of a column. There is a _function_ (namely `np.log`), but this isn't a tranformer -- it doesn't have a `fit` or `transform` method. \n",
    "\n",
    "The problem of wanting to apply a function to a column which isn't _technically_ a transformer is common, so Scikit-learn introduced a [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html). A `FunctionTransformer` takes a function (such as `np.log`) and makes a transformer that does nothing when `fit` is called, but calls the function when `transform` is called. It is easy to use.\n",
    "\n",
    "Let's see how it gets used in our example. We don't have to worry about taking the log of 0, we will take `np.log1p` of the `log_features` rather than `np.log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:50:36.321348Z",
     "start_time": "2019-05-25T17:50:36.318035Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log', FunctionTransformer(np.log1p, validate=False), log_features),\n",
    "        ('scale', StandardScaler(), scale_features)],\n",
    "    remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! It took far longer to describe in words than to code. By default, a `ColumnTransformer` will move all features that were not transformed, but this can be overriden (as it was here) by using the `remainder=\"passthrough\"` argument.\n",
    "\n",
    "Now let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T17:56:11.665397Z",
     "start_time": "2019-05-25T17:56:11.581514Z"
    }
   },
   "outputs": [],
   "source": [
    "log_reg = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('logisitic', LogisticRegression(C=0.02, solver='lbfgs', max_iter=10000, class_weight='balanced'))\n",
    "]).fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! The model's performance isn't that great -- looking at accuracy and recall show pretty poor results (note that the default rate is about 16%, but we did use class weight's to increase accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T18:01:22.817873Z",
     "start_time": "2019-05-25T18:01:22.787471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.634098</td>\n",
       "      <td>0.604733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.635626</td>\n",
       "      <td>0.604972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy    Recall\n",
       "Train  0.634098  0.604733\n",
       "Test   0.635626  0.604972"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore\n",
    "pd.DataFrame([\n",
    "    [log_reg.score(X_train, y_train), recall_score(y_train, log_reg.predict(X_train))],\n",
    "    [log_reg.score(X_test, y_test), recall_score(y_test, log_reg.predict(X_test))],\n",
    "], columns = ['Accuracy', 'Recall'], index=['Train', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T18:00:54.673447Z",
     "start_time": "2019-05-25T18:00:54.668322Z"
    }
   },
   "source": [
    "The poor results are due to the number of informative features we threw away in order to keep this example self-contained (as well as our use of Logistic Regression, instead of a tree-based model which would require less preprocessing). That is okay; our goal in this article was to show how to use the `ColumnTransformer` in non-trivial problems, rather than to build a good classifier. \n",
    "\n",
    "We will revisit this dataset where we will use `CategoryEncoders` along with the `ColumnTransformer` to make a much better model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* `ColumnTransformer` is written a lot like a pipeline, except when specifying the steps we also specify which columns should be transformed\n",
    "* This allows us to scale some features, while not scaling the OneHotEncoded or binary features (this makes interpreting the final coefficient of these features simpler, as the values are still 0 or 1, so the coefficient is the contribution of this feature being present vs absent)\n",
    "* It also allows you to choose different imputation methods on a column-by-column basis\n",
    "* By placing the `ColumnTransformer` in a pipeline with your model, you can easily do your preprocessing inside `GridSearchCV` and not worry about data leakage. The alternatives are either to transform the entire training set (which has data leak into your validation set, making CV scores too optimistic) or manually doing your cross-validation. For _most_ types of preprocessing, this isn't a huge issue (except if upsampling, as discussed [here](/how-to-do-cross-validation-when-upsampling-data.html)), but it is nice that the `ColumnTransformer` makes it easy to do the right thing.\n",
    "\n",
    "Of course, categorical variables have their own set of challenges and transformations. To tackle categorical variables, we will to introduce the `CategoryEncoders` package.\n",
    "\n",
    "## Related articles\n",
    "\n",
    "#### On my blog\n",
    "\n",
    "* [An introduction to pipeslines](): What they are, and why you should be using them (to come)\n",
    "* [How to do cross-validation when upsampling](/how-to-do-cross-validation-when-upsampling-data.html) An example of data leakage in cross-validation that is important.\n",
    "* [Why `pd.get_dummies` is Evil, use CategoryEncoders instead](): The problems of using `get_dummies` and how the new CategoryEncoders package addresses these issues (to come)\n",
    "* [Imputation by Group](): How to imputation on groups (to come)\n",
    "\n",
    "#### Elsewhere\n",
    "\n",
    "* [Scikit-learn tutorial on ColumnTransformers](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
