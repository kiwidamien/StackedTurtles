{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my colleagues, Sophie Searcy, recently wrote an [blog post](https://soph.info/2019/05/07/imbalance/) that dealt with imbalanced classes. She looked at ways to address an imbalanced learning problem, as well as the pros and cons of the different approaches. One of the big takeaways of that article (which you should read!) was to carefully consider whether or not you should address the problem of imbalance by oversampling, or if you should look at some of the alternatives: adjusting the weights of the classes, or checking if your model deals with imbalanced data naturally.\n",
    "\n",
    "This article is about how to do cross-validation once you have decided that oversampling is the right approach for your problem. This article is available as a [notebook](https://gist.github.com/kiwidamien/bcbe8e527a5f0cc9f28c4fe692f70cbc) on Github with all steps included; this article highlights the main steps.\n",
    "\n",
    "For this article, we will be going through the following steps:\n",
    "<ol>\n",
    "    <li> Getting a baseline</li>\n",
    "<li> Oversampling the wrong way<br/>\n",
    "   Do a train-test split, then oversample, then cross-validate. Sounds fine, but results are overly optimistic.</li>\n",
    "<li> Oversampling the right way\n",
    "    <ol>\n",
    "        <li type='a'> Manual oversampling</li>\n",
    "        <li type='a'> Using `imblearn`'s pipelines (for those in a hurry, this is the best solution)</li>\n",
    "    </ol>\n",
    "    </li>\n",
    "    </ol>\n",
    "\n",
    "We will see if cross-validation is done on already upsampled data, the scores don't generalize to new data. In a real problem, you should only use the test set **ONCE**; we are reusing it to show that if we do cross-validation on already upsampled data, the results are overly optimistic and do not generalize to new data (or the test set).\n",
    "\n",
    "\n",
    "### The dataset\n",
    "\n",
    "We will be using a thyroid dataset, where the number of bad thyroids make up about 6% of the data (i.e. about 1 in 16 patients have thyroid issues). The dataset is available as part of the imbalanced learn's dataset module. Our goal will be to find a classifier with a good recall (i.e.  we want our classifier to find as many positive cases as it can). We have to be aware there is a danger in using this metric, as simply predicting _everyone_ has a bad thyroid will make the recall 100%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:00.882280Z",
     "start_time": "2019-05-25T06:51:58.927476Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn import datasets\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:01.451385Z",
     "start_time": "2019-05-25T06:52:00.885119Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore\n",
    "\n",
    "thyroid_collection = datasets.fetch_datasets()['thyroid_sick']\n",
    "X = pd.DataFrame(thyroid_collection['data'])\n",
    "y = thyroid_collection['target']\n",
    "y[y==-1] = 0\n",
    "\n",
    "# Now that we have zeroed out the \"normal\" cases, we can find imbalance using \n",
    "imbalance_frac = y.mean()\n",
    "# imbalance_frac = 0.0612"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to ensure that we have the same splits of the data every time. We can ensure this by creating a `KFold` object, `kf`, and passing `cv=kf` instead of the more common `cv=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:01.456445Z",
     "start_time": "2019-05-25T06:52:01.453837Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline (no oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a baseline result by picking a random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:02.225084Z",
     "start_time": "2019-05-25T06:52:01.458764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81081081, 0.73684211, 0.875     , 0.7037037 , 0.7804878 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=45)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=13)\n",
    "cross_val_score(rf, X_train, y_train, cv=kf, scoring='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are decent results, and we haven't even optimized the model! Let's do some hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:13.663457Z",
     "start_time": "2019-05-25T06:52:02.227261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7803820054409211"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [4, 6, 10, 12],\n",
    "    'random_state': [13]\n",
    "}\n",
    "\n",
    "grid_no_up = GridSearchCV(rf, param_grid=params, cv=kf, \n",
    "                          scoring='recall').fit(X_train, y_train)\n",
    "\n",
    "grid_no_up.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 78% recall on one of our models before we have tried oversampling. This is the number to beat.\n",
    "\n",
    "Normally we would wait until we had finished our modeling to look at the test set, but an important part of this is to see how oversampling, done incorrectly, can make us too confident in our ability to generalize based off cross-validation. We haven't oversampled yet, so let's just check that the test scores are in line with what we expect from the CV scores about (i.e. about 78%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:13.709249Z",
     "start_time": "2019-05-25T06:52:13.666163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8035714285714286"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, grid_no_up.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it is (roughly) consistent with the CV results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Oversampling (the wrong way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just oversample the training data (we are smart enough not to oversample the test data), and check that this gives us an even split of the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:13.742680Z",
     "start_time": "2019-05-25T06:52:13.712171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_upsample, y_train_upsample = SMOTE(random_state=42).fit_sample(X_train, y_train)\n",
    "y_train_upsample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's cross-validate using grid search. Remember the training set has been upsampled; that is _not_ being done as part of the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:35.771751Z",
     "start_time": "2019-05-25T06:52:13.746028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9843160927198451"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [4, 6, 10, 12],\n",
    "    'random_state': [13]\n",
    "}\n",
    "\n",
    "grid_naive_up = GridSearchCV(rf, param_grid=params, cv=kf, \n",
    "                             scoring='recall').fit(X_train_upsample, \n",
    "                                                   y_train_upsample)\n",
    "grid_naive_up.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an amazing recall! If we look at the validation scores, they _all_ look pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:35.779603Z",
     "start_time": "2019-05-25T06:52:35.775098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93360792, 0.9345499 , 0.93337591, 0.94714925, 0.94736138,\n",
       "       0.94273667, 0.97585677, 0.98218414, 0.97864618, 0.98237253,\n",
       "       0.98187974, 0.98431609])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_naive_up.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the model that made these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:35.785556Z",
     "start_time": "2019-05-25T06:52:35.781880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 12, 'n_estimators': 200, 'random_state': 13}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_naive_up.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's look at how it does on the training set as a whole (once we eliminate the upsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:35.846444Z",
     "start_time": "2019-05-25T06:52:35.787750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_train, grid_naive_up.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what about the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:35.880619Z",
     "start_time": "2019-05-25T06:52:35.848389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107142857142857"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But wait ... uh-oh, spagetti-os!\n",
    "recall_score(y_test, grid_naive_up.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, time for some good news/bad news:\n",
    "- good: the recall on the test set is 91%, better than the 80% we got without upsampling\n",
    "- bad: our confidence in the cross-valdation results went down. With no upsampling, the validation recall was 78%, which was a good estimate of the test validation of 80%. With upsampling, the validation recall was 100% which isn't a good measure of the test recall (91%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's make SMOTE-ing part of our cross validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that we \n",
    "- oversample\n",
    "- then split into cross-validation folds\n",
    "\n",
    "To see why this is an issue, consider the simplest method of over-sampling (namely, copying the data point). Let's say every data point from the minority class is copied 6 times before making the splits. If we did a 3-fold validation, each fold has (on average) 2 copies of each point! If our classifier overfits by memorizing its training set, it should be able to get a perfect score on the validation set! Our cross-validation will choose the model that overfits the most. We see that CV chose the deepest trees it could!\n",
    "\n",
    "Instead, we should split into training and validation folds. Then, on each fold, we should\n",
    "1. Oversample the minority class\n",
    "2. Train the classifier on the training folds\n",
    "3. Validate the classifier on the remaining fold\n",
    "\n",
    "Let's see this in detail by doing it manually:\n",
    "\n",
    "### 3A. Manual upsampling within folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:37.086826Z",
     "start_time": "2019-05-25T06:52:35.882335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78378378, 0.76315789, 0.96875   , 0.81481481, 0.90243902])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 5,\n",
    "        'random_state': 13\n",
    "    }\n",
    "\n",
    "def score_model(model, params, cv=None):\n",
    "    \"\"\"\n",
    "    Creates folds manually, and upsamples within each fold.\n",
    "    Returns an array of validation (recall) scores\n",
    "    \"\"\"\n",
    "    if cv is None:\n",
    "        cv = KFold(n_splits=5, random_state=42)\n",
    "\n",
    "    smoter = SMOTE(random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for train_fold_index, val_fold_index in cv.split(X_train, y_train):\n",
    "        # Get the training data\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_fold_index], y_train[train_fold_index]\n",
    "        # Get the validation data\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_fold_index], y_train[val_fold_index]\n",
    "\n",
    "        # Upsample only the data in the training section\n",
    "        X_train_fold_upsample, y_train_fold_upsample = smoter.fit_resample(X_train_fold,\n",
    "                                                                           y_train_fold)\n",
    "        # Fit the model on the upsampled training data\n",
    "        model_obj = model(**params).fit(X_train_fold_upsample, y_train_fold_upsample)\n",
    "        # Score the model on the (non-upsampled) validation data\n",
    "        score = recall_score(y_val_fold, model_obj.predict(X_val_fold))\n",
    "        scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "# Example of the model in action\n",
    "score_model(RandomForestClassifier, example_params, cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even do grid search this way by looping over the parameters. As a reminder, the parameter combinations we tried earlier were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:37.094979Z",
     "start_time": "2019-05-25T06:52:37.090002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [50, 100, 200],\n",
       " 'max_depth': [4, 6, 10, 12],\n",
       " 'random_state': [13]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop tries all combinations, and stores the average recall score on the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:56.818587Z",
     "start_time": "2019-05-25T06:52:37.097465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50,\n",
       " 'max_depth': 4,\n",
       " 'random_state': 13,\n",
       " 'recall': 0.8486884268736002}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_tracker = []\n",
    "for n_estimators in params['n_estimators']:\n",
    "    for max_depth in params['max_depth']:\n",
    "        example_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'random_state': 13\n",
    "        }\n",
    "        example_params['recall'] = score_model(RandomForestClassifier, \n",
    "                                               example_params, cv=kf).mean()\n",
    "        score_tracker.append(example_params)\n",
    "        \n",
    "# What's the best model?\n",
    "sorted(score_tracker, key=lambda x: x['recall'], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator appears to have the parameters\n",
    "```\n",
    "{'n_estimators': 50,\n",
    "  'max_depth': 4,\n",
    "  'random_state': 13,\n",
    " }\n",
    "```\n",
    "and a recall score of 85% for the validation score. Let's see how this compares with the test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:56.949623Z",
     "start_time": "2019-05-25T06:52:56.820416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8392857142857143"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, max_depth=4, random_state=13)\n",
    "rf.fit(X_train_upsample, y_train_upsample)\n",
    "recall_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that is is roughly consistent (84% vs 85%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Let's use the imbalanced class pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imbalanced-learn dataset extends the sklearn's built-in pipeline methods. Specifically, you can import \n",
    "```python\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "```\n",
    "which will allow you to do multiple steps at once. It is also nice that if you _fit_ the model, all the steps (such as scaling, and the model) are fit at once. If you _predict_ with the model, scaling steps are only _trensformed_, so you can pass multiple steps into a pipeline. \n",
    "\n",
    "There is a restriction. The restriction comes partially from the naming of functions (e.g. `transform` vs `resample`) but one way of thing of it is that sklearn's pipeline only allows for one row in to be transformed to another row (perhaps with different or added features). To upsample, we need to _increase_ the number of rows. Imbalanced-learn generalizes the pipeline, but tries to keep the syntax and function names the same:\n",
    "```python\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "```\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:52:58.720254Z",
     "start_time": "2019-05-25T06:52:56.951951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75675676, 0.78947368, 0.90625   , 0.77777778, 0.7804878 ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imba_pipeline = make_pipeline(SMOTE(random_state=42), \n",
    "                              RandomForestClassifier(n_estimators=100, random_state=13))\n",
    "cross_val_score(imba_pipeline, X_train, y_train, scoring='recall', cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T04:43:46.574705Z",
     "start_time": "2019-05-25T04:43:44.916253Z"
    }
   },
   "source": [
    "This is much nicer than using our manual score function! Notice that the recall scores are similar to when we did this manually.\n",
    "\n",
    "Even nicer, the pipelines play well with `GridSearchCV`, so we don't have to loop over parameters manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:53:19.698549Z",
     "start_time": "2019-05-25T06:52:58.722068Z"
    }
   },
   "outputs": [],
   "source": [
    "new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best estimator selected by grid search with the pipeline matches the one we found manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:53:19.704256Z",
     "start_time": "2019-05-25T06:53:19.700327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__max_depth': 4,\n",
       " 'randomforestclassifier__n_estimators': 50,\n",
       " 'randomforestclassifier__random_state': 13}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_imba.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do we do on our validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:53:19.711743Z",
     "start_time": "2019-05-25T06:53:19.706685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8486780485230826"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_imba.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:53:19.725257Z",
     "start_time": "2019-05-25T06:53:19.713970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8392857142857143"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = grid_imba.predict(X_test)\n",
    "recall_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some confidence this is doing what we want: when we did cross-validation manually, we also saw cross-validation give recall scores of 85% (vs 84% recall on the test set).\n",
    "\n",
    "When predicting, the SMOTE step doesn't do anything (it just passes the values through). We can check this explicitly by just making a prediction from the `randomforestclassifier` and seeing we get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T06:53:19.740454Z",
     "start_time": "2019-05-25T06:53:19.728519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8392857142857143"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predict = grid_imba.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)\n",
    "recall_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here is a summary of the different approaches we took:\n",
    "\n",
    "| Method | Recall (validation) | Recall (test) |\n",
    "| --- | --- | --- |\n",
    "| No upsampling (baseline) | 78.0% | 80.3% |\n",
    "| Upsample training set before CV | 100% | 91.1% |\n",
    "| Upsample as part of CV (manual) | 84.9% | 83.9% |\n",
    "| Upsample as part of CV (pipeline) | 84.9% | 83.9% |\n",
    "\n",
    "The last two lines should be (and are) the same. The difference is simple the pipeline is easier to manage and leads to cleaner code, but it is good to see the explicit process once. The high level takeaways:\n",
    "\n",
    "* For each case, except when we upsampled the training set before the CV, the validation set recall was a good estimate of the test set recall.\n",
    "* When we upsampled the training set before cross validation, there was a difference of **9** percentage points between the CV recall and recall on the test set.\n",
    "* When upsampling before cross validation, you will be picking the most oversampled model, because the oversampling is allowing data to leak from the validation folds into the training folds.\n",
    "* In _this example_ doing the upsampling incorrectly lead to the best recall overall (91%). This won't generally happen! Our metric (recall) could have been much worse. The important point is that the main way we have of telling if we are doing well is using the CV scores.\n",
    "* The test set should only be used **ONCE**. In this article, we used it multiple times to show when how the different upsampling method affected our ability to trust the cross-validated scores. \n",
    "\n",
    "In your problems, you should do your baseline model and the (correctly) upsampled models, and use the CV scores for your modeling decisions. The test set's role is to tell how well your model generalizes after making all of your modeling decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
