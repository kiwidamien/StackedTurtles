{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "This creates the fake data for the colorblind example (JamesStein Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "numbers = {\n",
    "    ('female', 'white'): (10, 2000),\n",
    "    ('male', 'white'): (150, 2000),\n",
    "    ('female', 'asian'): (10, 2000),\n",
    "    ('male', 'asian'): (90, 2000),\n",
    "    ('female', 'black'): (5, 975),\n",
    "    ('male', 'black'): (37, 975),\n",
    "    ('male', 'hispanic'): (2, 50)\n",
    "}\n",
    "\n",
    "frames = []\n",
    "\n",
    "for gender, race in numbers:\n",
    "    num_colorblind, num_total = numbers[(gender, race)]\n",
    "    weights = np.round(np.random.normal(loc=160, scale=8, size=num_total), 2)\n",
    "    ages = np.random.binomial(90, 0.25)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'gender': gender,\n",
    "        'race': race,\n",
    "        'age': ages,\n",
    "        'height': weights,\n",
    "        'Colorblind': [True]*num_colorblind + [False]*(num_total - num_colorblind)\n",
    "    })\n",
    "    frames.append(df)\n",
    "    \n",
    "dataset = pd.concat(frames).sample(frac=1.0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>Colorblind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "      <td>29</td>\n",
       "      <td>145.95</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>black</td>\n",
       "      <td>28</td>\n",
       "      <td>161.83</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>asian</td>\n",
       "      <td>21</td>\n",
       "      <td>160.87</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>21</td>\n",
       "      <td>164.64</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>asian</td>\n",
       "      <td>21</td>\n",
       "      <td>158.88</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   race  age  height  Colorblind\n",
       "0  female  black   29  145.95       False\n",
       "1    male  black   28  161.83       False\n",
       "2    male  asian   21  160.87       False\n",
       "3  female  white   21  164.64       False\n",
       "4    male  asian   21  158.88       False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('colorblind.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on the test set is 0.34285714285714286\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Some fake data loaded from Github\n",
    "colorblind = dataset\n",
    "\n",
    "# Do the train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(colorblind.drop('Colorblind', axis=1), colorblind['Colorblind'])\n",
    "\n",
    "# Build the encoder\n",
    "encoder = ce.JamesSteinEncoder(cols=['gender', 'race'], return_df=True)\n",
    "\n",
    "# Build the model, including the encoder\n",
    "model = Pipeline([\n",
    "  ('encode_categorical', encoder),\n",
    "  ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Here are the parameters we want to search over\n",
    "# Review pipelines to see how to access the different \n",
    "# stages\n",
    "params = {\n",
    "  'classifier__n_estimators': [50, 100, 200],\n",
    "  'classifier__max_depth': [4, 6, 8],\n",
    "  'classifier__class_weight': [{0: 1, 1: 20}]\n",
    "}\n",
    "\n",
    "# build a grid search\n",
    "grid = GridSearchCV(model, param_grid=params, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# How well did we do on the test set?\n",
    "# Note that we don't need to explicitly transform the test\n",
    "# set!\n",
    "predict_test = grid.predict(X_test)\n",
    "print(f\"Recall on the test set is {recall_score(y_test, predict_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.pipeline import Pipeline as skPipeline\n",
    "\n",
    "# One-Hot Encoder for Pipeline\n",
    "onehot_encoder = ce.OneHotEncoder(\n",
    "     cols = ['listing_region',\n",
    "             'engine_cylinder_layout',\n",
    "             'engine_aspiration',\n",
    "             'fuel_type',\n",
    "             'drivetrain',\n",
    "             'transmission_gearbox',\n",
    "             'engine_type',\n",
    "             'body_type',\n",
    "             'buyer_source'],\n",
    "     use_cat_names=True\n",
    ")\n",
    "\n",
    "# James-Stein Encoding for columns with many unique values\n",
    "JS_encoding = ce.JamesSteinEncoder(\n",
    "    cols=['trim', 'make', 'model',\n",
    "          'exterior_color', 'interior_color']\n",
    ")\n",
    "\n",
    "class MyProcessor:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self.pipeline.fit(X, y)\n",
    "        \n",
    "    def transform(self, X, y):\n",
    "        return self.pipeline.transform(X, y)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.pipeline.fit_tranform(X, y)\n",
    "    \n",
    "    \n",
    "\n",
    "# Encoders into Pipeline\n",
    "categorical_pipeline = MyProcessor(\n",
    "    skPipeline([\n",
    "        ('one_hot_encode', onehot_encoder),\n",
    "        ('JS_encode', JS_encoding)\n",
    "    ])\n",
    ")\n",
    "\n",
    "\n",
    "# make_column_transfomer is a helper function\n",
    "# to make a pipeline that only operates on some columns\n",
    "process_numeric = make_column_transformer(\n",
    "    (FunctionTransformer(np.log1p, validate=False), \n",
    "     ['is_lease', 'num_accidents', 'is_manual_transmission', 'percent_to_market',]),\n",
    "    (FunctionTransformer(np.log, validate=False), \n",
    "     ['engine_displacement_liters', 'fuel_economy_city', 'quoted_list_price', \n",
    "      'engine_cylinder_count', 'num_owners', 'num_doors']),\n",
    "    (StandardScaler(), ['quote_month', 'quote_year', 'fuel_economy_highway','years_old', 'odometer']),\n",
    "    remainder='passthrough' \n",
    "    # must have 'passthrough', otherwise all the non-tranformed (i.e. categorical) features will be dropped!\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('transform_categorical', categorical_pipeline),\n",
    "    ('transform_numeric', process_numeric),\n",
    "    ('oversampling', RandomOverSampler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_steps(self):\n",
    "    names, estimators = zip(*self.steps)\n",
    "\n",
    "    # validate names\n",
    "    self._validate_names(names)\n",
    "\n",
    "    # validate estimators\n",
    "    transformers = estimators[:-1]\n",
    "    estimator = estimators[-1]\n",
    "\n",
    "    for t in transformers:\n",
    "        if t is None or t == 'passthrough':\n",
    "            continue\n",
    "        if (not (hasattr(t, \"fit\") or\n",
    "                 hasattr(t, \"fit_transform\") or\n",
    "                 hasattr(t, \"fit_resample\")) or\n",
    "                not (hasattr(t, \"transform\") or\n",
    "                     hasattr(t, \"fit_resample\"))):\n",
    "            raise TypeError(\n",
    "                \"All intermediate steps of the chain should \"\n",
    "                \"be estimators that implement fit and transform or \"\n",
    "                \"fit_resample (but not both) or be a string 'passthrough' \"\n",
    "                \"'%s' (type %s) doesn't)\" % (t, type(t)))\n",
    "\n",
    "        if (hasattr(t, \"fit_resample\") and (hasattr(t, \"fit_transform\") or\n",
    "                                            hasattr(t, \"transform\"))):\n",
    "            raise TypeError(\n",
    "                \"All intermediate steps of the chain should \"\n",
    "                \"be estimators that implement fit and transform or sample.\"\n",
    "                \" '%s' implements both)\" % (t))\n",
    "\n",
    "        if isinstance(t, pipeline.Pipeline):\n",
    "            raise TypeError(\n",
    "                \"All intermediate steps of the chain should not be\"\n",
    "                \" Pipelines\")\n",
    "\n",
    "    # We allow last estimator to be None as an identity transformation\n",
    "    if (estimator is not None and estimator != 'passthrough'\n",
    "            and not hasattr(estimator, \"fit\")):\n",
    "        raise TypeError(\"Last step of Pipeline should implement fit or be \"\n",
    "                        \"the string 'passthrough'. '%s' (type %s) doesn't\"\n",
    "                        % (estimator, type(estimator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_transformer = preprocess_pipeline.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(preprocess_pipeline, skPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\"\"\"\r\n",
      "The :mod:`imblearn.pipeline` module implements utilities to build a\r\n",
      "composite estimator, as a chain of transforms, samples and estimators.\r\n",
      "\"\"\"\r\n",
      "# Adapted from scikit-learn\r\n",
      "\r\n",
      "# Author: Edouard Duchesnay\r\n",
      "#         Gael Varoquaux\r\n",
      "#         Virgile Fritsch\r\n",
      "#         Alexandre Gramfort\r\n",
      "#         Lars Buitinck\r\n",
      "#         Christos Aridas\r\n",
      "#         Guillaume Lemaitre <g.lemaitre58@gmail.com>\r\n",
      "# License: BSD\r\n",
      "\r\n",
      "from collections import defaultdict\r\n",
      "from itertools import islice\r\n",
      "\r\n",
      "from sklearn import pipeline\r\n",
      "from sklearn.base import clone\r\n",
      "from sklearn.utils.metaestimators import if_delegate_has_method\r\n",
      "from sklearn.utils.validation import check_memory\r\n",
      "\r\n",
      "__all__ = ['Pipeline', 'make_pipeline']\r\n",
      "\r\n",
      "\r\n",
      "class Pipeline(pipeline.Pipeline):\r\n",
      "    \"\"\"Pipeline of transforms and resamples with a final estimator.\r\n",
      "\r\n",
      "    Sequentially apply a list of transforms, sampling, and a final estimator.\r\n",
      "    Intermediate steps of the pipeline must be transformers or resamplers,\r\n",
      "    that is, they must implement fit, transform and sample methods.\r\n",
      "    The samplers are only applied during fit.\r\n",
      "    The final estimator only needs to implement fit.\r\n",
      "    The transformers and samplers in the pipeline can be cached using\r\n",
      "    ``memory`` argument.\r\n",
      "\r\n",
      "    The purpose of the pipeline is to assemble several steps that can be\r\n",
      "    cross-validated together while setting different parameters.\r\n",
      "    For this, it enables setting parameters of the various steps using their\r\n",
      "    names and the parameter name separated by a '__', as in the example below.\r\n",
      "    A step's estimator may be replaced entirely by setting the parameter\r\n",
      "    with its name to another estimator, or a transformer removed by setting\r\n",
      "    it to 'passthrough' or ``None``.\r\n",
      "\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    steps : list\r\n",
      "        List of (name, transform) tuples (implementing\r\n",
      "        fit/transform/fit_resample) that are chained, in the order in which\r\n",
      "        they are chained, with the last object an estimator.\r\n",
      "\r\n",
      "    memory : Instance of joblib.Memory or string, optional (default=None)\r\n",
      "        Used to cache the fitted transformers of the pipeline. By default,\r\n",
      "        no caching is performed. If a string is given, it is the path to\r\n",
      "        the caching directory. Enabling caching triggers a clone of\r\n",
      "        the transformers before fitting. Therefore, the transformer\r\n",
      "        instance given to the pipeline cannot be inspected\r\n",
      "        directly. Use the attribute ``named_steps`` or ``steps`` to\r\n",
      "        inspect estimators within the pipeline. Caching the\r\n",
      "        transformers is advantageous when fitting is time consuming.\r\n",
      "\r\n",
      "\r\n",
      "    Attributes\r\n",
      "    ----------\r\n",
      "    named_steps : dict\r\n",
      "        Read-only attribute to access any step parameter by user given name.\r\n",
      "        Keys are step names and values are steps parameters.\r\n",
      "\r\n",
      "    Notes\r\n",
      "    -----\r\n",
      "    See :ref:`sphx_glr_auto_examples_pipeline_plot_pipeline_classification.py`\r\n",
      "\r\n",
      "    See also\r\n",
      "    --------\r\n",
      "    make_pipeline : helper function to make pipeline.\r\n",
      "\r\n",
      "    Examples\r\n",
      "    --------\r\n",
      "\r\n",
      "    >>> from collections import Counter\r\n",
      "    >>> from sklearn.datasets import make_classification\r\n",
      "    >>> from sklearn.model_selection import train_test_split as tts\r\n",
      "    >>> from sklearn.decomposition import PCA\r\n",
      "    >>> from sklearn.neighbors import KNeighborsClassifier as KNN\r\n",
      "    >>> from sklearn.metrics import classification_report\r\n",
      "    >>> from imblearn.over_sampling import SMOTE\r\n",
      "    >>> from imblearn.pipeline import Pipeline # doctest: +NORMALIZE_WHITESPACE\r\n",
      "    >>> X, y = make_classification(n_classes=2, class_sep=2,\r\n",
      "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\r\n",
      "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\r\n",
      "    >>> print('Original dataset shape {}'.format(Counter(y)))\r\n",
      "    Original dataset shape Counter({1: 900, 0: 100})\r\n",
      "    >>> pca = PCA()\r\n",
      "    >>> smt = SMOTE(random_state=42)\r\n",
      "    >>> knn = KNN()\r\n",
      "    >>> pipeline = Pipeline([('smt', smt), ('pca', pca), ('knn', knn)])\r\n",
      "    >>> X_train, X_test, y_train, y_test = tts(X, y, random_state=42)\r\n",
      "    >>> pipeline.fit(X_train, y_train) # doctest: +ELLIPSIS\r\n",
      "    Pipeline(...)\r\n",
      "    >>> y_hat = pipeline.predict(X_test)\r\n",
      "    >>> print(classification_report(y_test, y_hat))\r\n",
      "                  precision    recall  f1-score   support\r\n",
      "    <BLANKLINE>\r\n",
      "               0       0.87      1.00      0.93        26\r\n",
      "               1       1.00      0.98      0.99       224\r\n",
      "    <BLANKLINE>\r\n",
      "        accuracy                           0.98       250\r\n",
      "       macro avg       0.93      0.99      0.96       250\r\n",
      "    weighted avg       0.99      0.98      0.98       250\r\n",
      "    <BLANKLINE>\r\n",
      "\r\n",
      "    \"\"\"\r\n",
      "\r\n",
      "    # BaseEstimator interface\r\n",
      "\r\n",
      "    def _validate_steps(self):\r\n",
      "        names, estimators = zip(*self.steps)\r\n",
      "\r\n",
      "        # validate names\r\n",
      "        self._validate_names(names)\r\n",
      "\r\n",
      "        # validate estimators\r\n",
      "        transformers = estimators[:-1]\r\n",
      "        estimator = estimators[-1]\r\n",
      "\r\n",
      "        for t in transformers:\r\n",
      "            if t is None or t == 'passthrough':\r\n",
      "                continue\r\n",
      "            if (not (hasattr(t, \"fit\") or\r\n",
      "                     hasattr(t, \"fit_transform\") or\r\n",
      "                     hasattr(t, \"fit_resample\")) or\r\n",
      "                    not (hasattr(t, \"transform\") or\r\n",
      "                         hasattr(t, \"fit_resample\"))):\r\n",
      "                raise TypeError(\r\n",
      "                    \"All intermediate steps of the chain should \"\r\n",
      "                    \"be estimators that implement fit and transform or \"\r\n",
      "                    \"fit_resample (but not both) or be a string 'passthrough' \"\r\n",
      "                    \"'%s' (type %s) doesn't)\" % (t, type(t)))\r\n",
      "\r\n",
      "            if (hasattr(t, \"fit_resample\") and (hasattr(t, \"fit_transform\") or\r\n",
      "                                                hasattr(t, \"transform\"))):\r\n",
      "                raise TypeError(\r\n",
      "                    \"All intermediate steps of the chain should \"\r\n",
      "                    \"be estimators that implement fit and transform or sample.\"\r\n",
      "                    \" '%s' implements both)\" % (t))\r\n",
      "\r\n",
      "            if isinstance(t, pipeline.Pipeline):\r\n",
      "                raise TypeError(\r\n",
      "                    \"All intermediate steps of the chain should not be\"\r\n",
      "                    \" Pipelines\")\r\n",
      "\r\n",
      "        # We allow last estimator to be None as an identity transformation\r\n",
      "        if (estimator is not None and estimator != 'passthrough'\r\n",
      "                and not hasattr(estimator, \"fit\")):\r\n",
      "            raise TypeError(\"Last step of Pipeline should implement fit or be \"\r\n",
      "                            \"the string 'passthrough'. '%s' (type %s) doesn't\"\r\n",
      "                            % (estimator, type(estimator)))\r\n",
      "\r\n",
      "    # Estimator interface\r\n",
      "\r\n",
      "    def _fit(self, X, y=None, **fit_params):\r\n",
      "        self.steps = list(self.steps)\r\n",
      "        self._validate_steps()\r\n",
      "        # Setup the memory\r\n",
      "        memory = check_memory(self.memory)\r\n",
      "\r\n",
      "        fit_transform_one_cached = memory.cache(_fit_transform_one)\r\n",
      "        fit_resample_one_cached = memory.cache(_fit_resample_one)\r\n",
      "\r\n",
      "        fit_params_steps = {name: {} for name, step in self.steps\r\n",
      "                            if step is not None}\r\n",
      "        for pname, pval in fit_params.items():\r\n",
      "            step, param = pname.split('__', 1)\r\n",
      "            fit_params_steps[step][param] = pval\r\n",
      "        for step_idx, name, transformer in self._iter(with_final=False):\r\n",
      "            if hasattr(memory, 'location'):\r\n",
      "                # joblib >= 0.12\r\n",
      "                if memory.location is None:\r\n",
      "                    # we do not clone when caching is disabled to\r\n",
      "                    # preserve backward compatibility\r\n",
      "                    cloned_transformer = transformer\r\n",
      "                else:\r\n",
      "                    cloned_transformer = clone(transformer)\r\n",
      "            elif hasattr(memory, 'cachedir'):\r\n",
      "                # joblib < 0.11\r\n",
      "                if memory.cachedir is None:\r\n",
      "                    # we do not clone when caching is disabled to\r\n",
      "                    # preserve backward compatibility\r\n",
      "                    cloned_transformer = transformer\r\n",
      "            else:\r\n",
      "                cloned_transformer = clone(transformer)\r\n",
      "            # Fit or load from cache the current transfomer\r\n",
      "            if (hasattr(cloned_transformer, \"transform\") or\r\n",
      "                    hasattr(cloned_transformer, \"fit_transform\")):\r\n",
      "                X, fitted_transformer = fit_transform_one_cached(\r\n",
      "                    cloned_transformer, None, X, y,\r\n",
      "                    **fit_params_steps[name])\r\n",
      "            elif hasattr(cloned_transformer, \"fit_resample\"):\r\n",
      "                X, y, fitted_transformer = fit_resample_one_cached(\r\n",
      "                    cloned_transformer, X, y, **fit_params_steps[name])\r\n",
      "            # Replace the transformer of the step with the fitted\r\n",
      "            # transformer. This is necessary when loading the transformer\r\n",
      "            # from the cache.\r\n",
      "            self.steps[step_idx] = (name, fitted_transformer)\r\n",
      "        if self._final_estimator == 'passthrough':\r\n",
      "            return X, y, {}\r\n",
      "        return X, y, fit_params_steps[self.steps[-1][0]]\r\n",
      "\r\n",
      "    def fit(self, X, y=None, **fit_params):\r\n",
      "        \"\"\"Fit the model\r\n",
      "\r\n",
      "        Fit all the transforms/samplers one after the other and\r\n",
      "        transform/sample the data, then fit the transformed/sampled\r\n",
      "        data using the final estimator.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Training data. Must fulfill input requirements of first step of the\r\n",
      "            pipeline.\r\n",
      "\r\n",
      "        y : iterable, default=None\r\n",
      "            Training targets. Must fulfill label requirements for all steps of\r\n",
      "            the pipeline.\r\n",
      "\r\n",
      "        **fit_params : dict of string -> object\r\n",
      "            Parameters passed to the ``fit`` method of each step, where\r\n",
      "            each parameter name is prefixed such that parameter ``p`` for step\r\n",
      "            ``s`` has key ``s__p``.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        self : Pipeline\r\n",
      "            This estimator\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        Xt, yt, fit_params = self._fit(X, y, **fit_params)\r\n",
      "        if self._final_estimator != 'passthrough':\r\n",
      "            self._final_estimator.fit(Xt, yt, **fit_params)\r\n",
      "        return self\r\n",
      "\r\n",
      "    def fit_transform(self, X, y=None, **fit_params):\r\n",
      "        \"\"\"Fit the model and transform with the final estimator\r\n",
      "\r\n",
      "        Fits all the transformers/samplers one after the other and\r\n",
      "        transform/sample the data, then uses fit_transform on\r\n",
      "        transformed data with the final estimator.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Training data. Must fulfill input requirements of first step of the\r\n",
      "            pipeline.\r\n",
      "\r\n",
      "        y : iterable, default=None\r\n",
      "            Training targets. Must fulfill label requirements for all steps of\r\n",
      "            the pipeline.\r\n",
      "\r\n",
      "        **fit_params : dict of string -> object\r\n",
      "            Parameters passed to the ``fit`` method of each step, where\r\n",
      "            each parameter name is prefixed such that parameter ``p`` for step\r\n",
      "            ``s`` has key ``s__p``.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        Xt : array-like, shape = [n_samples, n_transformed_features]\r\n",
      "            Transformed samples\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        last_step = self._final_estimator\r\n",
      "        Xt, yt, fit_params = self._fit(X, y, **fit_params)\r\n",
      "        if last_step == 'passthrough':\r\n",
      "            return Xt\r\n",
      "        elif hasattr(last_step, 'fit_transform'):\r\n",
      "            return last_step.fit_transform(Xt, yt, **fit_params)\r\n",
      "        else:\r\n",
      "            return last_step.fit(Xt, yt, **fit_params).transform(Xt)\r\n",
      "\r\n",
      "    def fit_resample(self, X, y=None, **fit_params):\r\n",
      "        \"\"\"Fit the model and sample with the final estimator\r\n",
      "\r\n",
      "        Fits all the transformers/samplers one after the other and\r\n",
      "        transform/sample the data, then uses fit_resample on transformed\r\n",
      "        data with the final estimator.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Training data. Must fulfill input requirements of first step of the\r\n",
      "            pipeline.\r\n",
      "\r\n",
      "        y : iterable, default=None\r\n",
      "            Training targets. Must fulfill label requirements for all steps of\r\n",
      "            the pipeline.\r\n",
      "\r\n",
      "        **fit_params : dict of string -> object\r\n",
      "            Parameters passed to the ``fit`` method of each step, where\r\n",
      "            each parameter name is prefixed such that parameter ``p`` for step\r\n",
      "            ``s`` has key ``s__p``.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        Xt : array-like, shape = [n_samples, n_transformed_features]\r\n",
      "            Transformed samples\r\n",
      "\r\n",
      "        yt : array-like, shape = [n_samples, n_transformed_features]\r\n",
      "            Transformed target\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        last_step = self._final_estimator\r\n",
      "        Xt, yt, fit_params = self._fit(X, y, **fit_params)\r\n",
      "        if last_step == 'passthrough':\r\n",
      "            return Xt\r\n",
      "        elif hasattr(last_step, 'fit_resample'):\r\n",
      "            return last_step.fit_resample(Xt, yt, **fit_params)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def predict(self, X, **predict_params):\r\n",
      "        \"\"\"Apply transformers/samplers to the data, and predict with the final\r\n",
      "        estimator\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        **predict_params : dict of string -> object\r\n",
      "            Parameters to the ``predict`` called at the end of all\r\n",
      "            transformations in the pipeline. Note that while this may be\r\n",
      "            used to return uncertainties from some models with return_std\r\n",
      "            or return_cov, uncertainties that are generated by the\r\n",
      "            transformations in the pipeline are not propagated to the\r\n",
      "            final estimator.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_pred : array-like\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter(with_final=False):\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        return self.steps[-1][-1].predict(Xt, **predict_params)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def fit_predict(self, X, y=None, **fit_params):\r\n",
      "        \"\"\"Applies fit_predict of last step in pipeline after transforms.\r\n",
      "\r\n",
      "        Applies fit_transforms of a pipeline to the data, followed by the\r\n",
      "        fit_predict method of the final estimator in the pipeline. Valid\r\n",
      "        only if the final estimator implements fit_predict.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Training data. Must fulfill input requirements of first step of\r\n",
      "            the pipeline.\r\n",
      "\r\n",
      "        y : iterable, default=None\r\n",
      "            Training targets. Must fulfill label requirements for all steps\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        **fit_params : dict of string -> object\r\n",
      "            Parameters passed to the ``fit`` method of each step, where\r\n",
      "            each parameter name is prefixed such that parameter ``p`` for step\r\n",
      "            ``s`` has key ``s__p``.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_pred : array-like\r\n",
      "        \"\"\"\r\n",
      "        Xt, yt, fit_params = self._fit(X, y, **fit_params)\r\n",
      "        return self.steps[-1][-1].fit_predict(Xt, yt, **fit_params)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def predict_proba(self, X):\r\n",
      "        \"\"\"Apply transformers/samplers, and predict_proba of the final\r\n",
      "        estimator\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_proba : array-like, shape = [n_samples, n_classes]\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter(with_final=False):\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        return self.steps[-1][-1].predict_proba(Xt)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def score_samples(self, X):\r\n",
      "        \"\"\"Apply transforms, and score_samples of the final estimator.\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_score : ndarray, shape (n_samples,)\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transformer in self._iter(with_final=False):\r\n",
      "            if hasattr(transformer, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transformer.transform(Xt)\r\n",
      "        return self.steps[-1][-1].score_samples(Xt)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def decision_function(self, X):\r\n",
      "        \"\"\"Apply transformers/samplers, and decision_function of the final\r\n",
      "        estimator\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_score : array-like, shape = [n_samples, n_classes]\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter(with_final=False):\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        return self.steps[-1][-1].decision_function(Xt)\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def predict_log_proba(self, X):\r\n",
      "        \"\"\"Apply transformers/samplers, and predict_log_proba of the final\r\n",
      "        estimator\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        y_score : array-like, shape = [n_samples, n_classes]\r\n",
      "\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter(with_final=False):\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        return self.steps[-1][-1].predict_log_proba(Xt)\r\n",
      "\r\n",
      "    @property\r\n",
      "    def transform(self):\r\n",
      "        \"\"\"Apply transformers/samplers, and transform with the final estimator\r\n",
      "\r\n",
      "        This also works where final estimator is ``None``: all prior\r\n",
      "        transformations are applied.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to transform. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        Xt : array-like, shape = [n_samples, n_transformed_features]\r\n",
      "        \"\"\"\r\n",
      "        # _final_estimator is None or has transform, otherwise attribute error\r\n",
      "        if self._final_estimator != 'passthrough':\r\n",
      "            self._final_estimator.transform\r\n",
      "        return self._transform\r\n",
      "\r\n",
      "    def _transform(self, X):\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter():\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        return Xt\r\n",
      "\r\n",
      "    @property\r\n",
      "    def inverse_transform(self):\r\n",
      "        \"\"\"Apply inverse transformations in reverse order\r\n",
      "\r\n",
      "        All estimators in the pipeline must support ``inverse_transform``.\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        Xt : array-like, shape = [n_samples, n_transformed_features]\r\n",
      "            Data samples, where ``n_samples`` is the number of samples and\r\n",
      "            ``n_features`` is the number of features. Must fulfill\r\n",
      "            input requirements of last step of pipeline's\r\n",
      "            ``inverse_transform`` method.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        Xt : array-like, shape = [n_samples, n_features]\r\n",
      "        \"\"\"\r\n",
      "        # raise AttributeError if necessary for hasattr behaviour\r\n",
      "        for _, _, transform in self._iter():\r\n",
      "            transform.inverse_transform\r\n",
      "        return self._inverse_transform\r\n",
      "\r\n",
      "    def _inverse_transform(self, X):\r\n",
      "        Xt = X\r\n",
      "        reverse_iter = reversed(list(self._iter()))\r\n",
      "        for _, _, transform in reverse_iter:\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.inverse_transform(Xt)\r\n",
      "        return Xt\r\n",
      "\r\n",
      "    @if_delegate_has_method(delegate='_final_estimator')\r\n",
      "    def score(self, X, y=None, sample_weight=None):\r\n",
      "        \"\"\"Apply transformers/samplers, and score with the final estimator\r\n",
      "\r\n",
      "        Parameters\r\n",
      "        ----------\r\n",
      "        X : iterable\r\n",
      "            Data to predict on. Must fulfill input requirements of first step\r\n",
      "            of the pipeline.\r\n",
      "\r\n",
      "        y : iterable, default=None\r\n",
      "            Targets used for scoring. Must fulfill label requirements for all\r\n",
      "            steps of the pipeline.\r\n",
      "\r\n",
      "        sample_weight : array-like, default=None\r\n",
      "            If not None, this argument is passed as ``sample_weight`` keyword\r\n",
      "            argument to the ``score`` method of the final estimator.\r\n",
      "\r\n",
      "        Returns\r\n",
      "        -------\r\n",
      "        score : float\r\n",
      "        \"\"\"\r\n",
      "        Xt = X\r\n",
      "        for _, _, transform in self._iter(with_final=False):\r\n",
      "            if hasattr(transform, \"fit_resample\"):\r\n",
      "                pass\r\n",
      "            else:\r\n",
      "                Xt = transform.transform(Xt)\r\n",
      "        score_params = {}\r\n",
      "        if sample_weight is not None:\r\n",
      "            score_params['sample_weight'] = sample_weight\r\n",
      "        return self.steps[-1][-1].score(Xt, y, **score_params)\r\n",
      "\r\n",
      "\r\n",
      "def _fit_transform_one(transformer, weight, X, y, **fit_params):\r\n",
      "    if hasattr(transformer, 'fit_transform'):\r\n",
      "        res = transformer.fit_transform(X, y, **fit_params)\r\n",
      "    else:\r\n",
      "        res = transformer.fit(X, y, **fit_params).transform(X)\r\n",
      "    # if we have a weight for this transformer, multiply output\r\n",
      "    if weight is None:\r\n",
      "        return res, transformer\r\n",
      "    return res * weight, transformer\r\n",
      "\r\n",
      "\r\n",
      "def _fit_resample_one(sampler, X, y, **fit_params):\r\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\r\n",
      "\r\n",
      "    return X_res, y_res, sampler\r\n",
      "\r\n",
      "\r\n",
      "def make_pipeline(*steps, **kwargs):\r\n",
      "    \"\"\"Construct a Pipeline from the given estimators.\r\n",
      "\r\n",
      "    This is a shorthand for the Pipeline constructor; it does not require, and\r\n",
      "    does not permit, naming the estimators. Instead, their names will be set\r\n",
      "    to the lowercase of their types automatically.\r\n",
      "\r\n",
      "    Parameters\r\n",
      "    ----------\r\n",
      "    *steps : list of estimators.\r\n",
      "\r\n",
      "    memory : None, str or object with the joblib.Memory interface, optional\r\n",
      "        Used to cache the fitted transformers of the pipeline. By default,\r\n",
      "        no caching is performed. If a string is given, it is the path to\r\n",
      "        the caching directory. Enabling caching triggers a clone of\r\n",
      "        the transformers before fitting. Therefore, the transformer\r\n",
      "        instance given to the pipeline cannot be inspected\r\n",
      "        directly. Use the attribute ``named_steps`` or ``steps`` to\r\n",
      "        inspect estimators within the pipeline. Caching the\r\n",
      "        transformers is advantageous when fitting is time consuming.\r\n",
      "\r\n",
      "    Returns\r\n",
      "    -------\r\n",
      "    p : Pipeline\r\n",
      "\r\n",
      "    See also\r\n",
      "    --------\r\n",
      "    imblearn.pipeline.Pipeline : Class for creating a pipeline of\r\n",
      "        transforms with a final estimator.\r\n",
      "\r\n",
      "    Examples\r\n",
      "    --------\r\n",
      "    >>> from sklearn.naive_bayes import GaussianNB\r\n",
      "    >>> from sklearn.preprocessing import StandardScaler\r\n",
      "    >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\r\n",
      "    ... # doctest: +NORMALIZE_WHITESPACE\r\n",
      "    Pipeline(memory=None,\r\n",
      "             steps=[('standardscaler',\r\n",
      "                     StandardScaler(copy=True, with_mean=True, with_std=True)),\r\n",
      "                    ('gaussiannb',\r\n",
      "                     GaussianNB(priors=None, var_smoothing=1e-09))],\r\n",
      "             verbose=False)\r\n",
      "    \"\"\"\r\n",
      "    memory = kwargs.pop('memory', None)\r\n",
      "    if kwargs:\r\n",
      "        raise TypeError('Unknown keyword arguments: \"{}\"'\r\n",
      "                        .format(list(kwargs.keys())[0]))\r\n",
      "    return Pipeline(pipeline._name_estimators(steps), memory=memory)\r\n"
     ]
    }
   ],
   "source": [
    "!cat /anaconda3/envs/blog_env/lib/python3.7/site-packages/imblearn/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TargetEncoder(cols=['gender', 'race'], drop_invariant=False,\n",
       "              handle_missing='value', handle_unknown='value',\n",
       "              min_samples_leaf=1, return_df=True, smoothing=1.0, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ce.TargetEncoder(cols=['gender', 'race'], return_df=True)\n",
    "encoder.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>160.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>158.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>169.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7413</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>163.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>162.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4602</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>165.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>29</td>\n",
       "      <td>156.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>152.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6589</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>152.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5838</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>147.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5758</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>165.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9621</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>168.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>148.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>157.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>161.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5463</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>149.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>151.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>172.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>29</td>\n",
       "      <td>156.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>159.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>147.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5611</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>173.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9223</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>162.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>161.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9219</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>158.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>155.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6149</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>155.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>147.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6454</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>159.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>155.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>154.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>162.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>180.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>166.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7328</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>161.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>157.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>161.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>149.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6208</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>156.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3213</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>157.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>158.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7634</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>143.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>29</td>\n",
       "      <td>159.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>28</td>\n",
       "      <td>151.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>159.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>169.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>155.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>168.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5269</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>23</td>\n",
       "      <td>152.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>160.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>21</td>\n",
       "      <td>151.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>158.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>140.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>29</td>\n",
       "      <td>162.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>0.056454</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>14</td>\n",
       "      <td>158.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>163.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>158.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>161.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7644</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>19</td>\n",
       "      <td>162.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>21</td>\n",
       "      <td>139.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender      race  age  height\n",
       "594   0.056454  0.038721   14  160.21\n",
       "5401  0.056454  0.038721   14  158.17\n",
       "375   0.056454  0.028477   21  169.52\n",
       "7413  0.005635  0.038721   21  163.68\n",
       "376   0.056454  0.038721   14  162.37\n",
       "...        ...       ...  ...     ...\n",
       "8513  0.005635  0.038721   21  163.39\n",
       "8653  0.005635  0.028477   19  158.51\n",
       "1110  0.005635  0.028477   19  161.57\n",
       "7644  0.005635  0.028477   19  162.88\n",
       "3032  0.005635  0.038721   21  139.57\n",
       "\n",
       "[7500 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog_env",
   "language": "python",
   "name": "blog_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
